{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8E32O55hjwm/YMnpdePja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipesayegg/cnn_homer_bart/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  IntroduÃ§Ã£o\n",
        "\n",
        "\n",
        "### ğŸ“Œ Checkpoint 05 â€“ ClassificaÃ§Ã£o de Imagens com Redes Neurais Convolucionais (CNN)\n",
        "\n",
        "Este projeto tem como objetivo aplicar os conhecimentos adquiridos sobre Redes Neurais Convolucionais (CNNs), utilizando o framework TensorFlow/Keras, para realizar a classificaÃ§Ã£o de imagens dos personagens Homer Simpson e Bart Simpson.\n",
        "\n",
        "A proposta Ã© construir um modelo de Deep Learning capaz de aprender padrÃµes visuais a partir de um conjunto de imagens previamente organizadas em pastas por classe (Homer e Bart), realizando o treinamento, validaÃ§Ã£o e posteriormente a previsÃ£o sobre novas imagens.\n",
        "\n",
        "Este checkpoint estÃ¡ estruturado em cinco etapas principais:\n",
        "1. DefiniÃ§Ã£o do problema  \n",
        "2. PrÃ©-processamento das imagens  \n",
        "3. CriaÃ§Ã£o e explicaÃ§Ã£o da arquitetura da CNN  \n",
        "4. AvaliaÃ§Ã£o do desempenho do modelo  \n",
        "5. Teste de previsÃ£o com uma imagem externa  \n",
        "\n",
        "O foco Ã© avaliar diferentes configuraÃ§Ãµes da rede neural e compreender como pequenos ajustes podem impactar na capacidade do modelo de identificar corretamente os personagens. Ao final, serÃ¡ feita uma previsÃ£o real com uma imagem nova obtida da internet para validar o desempenho do modelo em dados fora da amostra.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Wy9i6bDpHgiX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "RkcQG9W4E2LB"
      },
      "outputs": [],
      "source": [
        "# ğŸ“š BIBLIOTECAS UTILIZADAS NESTE PROJETO\n",
        "\n",
        "# ğŸ”§ Ferramentas do TensorFlow para criar, treinar e avaliar a rede neural\n",
        "from tensorflow.keras.models import Sequential  # Permite criar o modelo de rede camada por camada\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout  # Camadas principais para CNN\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Gera as imagens e aplica prÃ©-processamentos automaticamente\n",
        "\n",
        "# ğŸ§® NumPy para manipulaÃ§Ã£o de arrays (como imagens) e preparaÃ§Ã£o de dados\n",
        "import numpy as np\n",
        "\n",
        "# ğŸ–¼ï¸ PIL (Python Imaging Library) Ã© usada para abrir e tratar imagens externas\n",
        "from PIL import Image\n",
        "\n",
        "# ğŸŒ Requests permite fazer o download de uma imagem da internet de forma simples\n",
        "import requests\n",
        "\n",
        "# ğŸ–¥ï¸ Se estiver usando o Google Colab, essas podem ser Ãºteis tambÃ©m:\n",
        "# from google.colab import drive  # Para montar o Google Drive (caso use dataset de lÃ¡)\n",
        "# import os  # Para navegar entre pastas, criar diretÃ³rios etc.\n",
        "\n",
        "# ğŸ’¬ ObservaÃ§Ã£o:\n",
        "# Todas essas bibliotecas sÃ£o bastante comuns em projetos com Deep Learning usando Keras/TensorFlow.\n",
        "# A ideia aqui Ã© manter o cÃ³digo limpo, compreensÃ­vel e modular, facilitando futuras adaptaÃ§Ãµes ou melhorias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1ï¸âƒ£ DefiniÃ§Ã£o do Problema\n",
        "\n",
        "Neste projeto, vamos treinar uma rede neural convolucional (CNN) para identificar qual personagem estÃ¡ em uma imagem: **Homer Simpson** ou **Bart Simpson**.\n",
        "\n",
        "A proposta Ã© usar um conjunto de imagens organizadas em pastas (uma para cada personagem) e treinar um modelo com TensorFlow/Keras capaz de reconhecer padrÃµes visuais, como formato do rosto, cor da roupa ou estilo do cabelo, que ajudem a diferenciar os dois personagens.\n",
        "\n",
        "SerÃ¡ usada uma divisÃ£o dos dados em treinamento e validaÃ§Ã£o, para que possamos medir o desempenho do modelo e evitar que ele apenas â€œdecoreâ€ os exemplos vistos.\n",
        "\n",
        "A CNN serÃ¡ avaliada com base na sua acurÃ¡cia (percentual de acertos) e tambÃ©m testada com uma imagem externa, vinda da internet, para verificar sua capacidade de generalizaÃ§Ã£o.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "GeBOvAs0Je5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. PrÃ©-processamento das imagens"
      ],
      "metadata": {
        "id": "nJUuGajuMzDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“š Bibliotecas para lidar com arquivos temporÃ¡rios e compactados\n",
        "import tempfile  # ğŸ”§ Cria pastas temporÃ¡rias â€” elas sÃ£o apagadas automaticamente ao final da execuÃ§Ã£o\n",
        "import zipfile   # ğŸ“¦ Permite abrir e extrair arquivos .zip\n",
        "\n",
        "# ğŸ—‚ï¸ Criando uma pasta temporÃ¡ria para extrair o conteÃºdo do zip\n",
        "temp_dir = tempfile.TemporaryDirectory()  # Isso cria uma pasta que sÃ³ existe enquanto o notebook estiver rodando\n"
      ],
      "metadata": {
        "id": "m_uNh8daI1Wl"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ğŸ”“ Abrindo o arquivo zip com as imagens dos personagens\n",
        "with zipfile.ZipFile('/content/dataset_personagens (1).zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(temp_dir.name)  # Extraindo todo o conteÃºdo para a pasta temporÃ¡ria\n",
        "\n",
        "# ğŸ” Visualizando o caminho da pasta extraÃ­da\n",
        "print(\"Imagens extraÃ­das em:\", temp_dir.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q6rxGb8KuOv",
        "outputId": "5cbdb8c7-7d70-4793-a4d4-65fe0cc59cfb"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagens extraÃ­das em: /tmp/tmpwb1y6qdy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. CriaÃ§Ã£o e explicaÃ§Ã£o da arquitetura da CNN"
      ],
      "metadata": {
        "id": "fKZ-YljcNBsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¨ Importando os \"blocos de montar\" da nossa rede neural\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "# ğŸ“š Modelo sequencial: permite empilhar camadas de forma linear (uma apÃ³s a outra)\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    InputLayer,         # ğŸ¯ Define a forma da entrada (opcional no modelo sequencial, mas Ãºtil para deixar claro o formato da imagem)\n",
        "    Conv2D,             # ğŸ§© Camada convolucional: identifica padrÃµes visuais como bordas, contornos e texturas\n",
        "    MaxPooling2D,       # ğŸ”½ Reduz a dimensÃ£o da imagem (ex: de 150x150 para 75x75), ajudando a evitar overfitting e economizar memÃ³ria\n",
        "    Flatten,            # ğŸ§· Transforma a imagem em vetor (necessÃ¡rio antes das camadas densas)\n",
        "    Dense,              # ğŸ”Œ Camada clÃ¡ssica de rede neural (totalmente conectada)\n",
        "    Dropout,            # ğŸ’§ Desativa aleatoriamente uma porcentagem dos neurÃ´nios durante o treino, prevenindo overfitting\n",
        "    BatchNormalization  # ğŸ§¼ Normaliza as ativaÃ§Ãµes entre as camadas, acelerando e estabilizando o aprendizado\n",
        ")\n",
        "\n",
        "# ğŸ–¼ï¸ Leitura de imagens individuais â€” Ãºtil para testar uma imagem depois do treino\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# ğŸ—‚ï¸ Leitura de imagens em massa a partir de pastas organizadas por classe\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# âš™ï¸ Ã‰ o gerador que prepara os dados para o treino e validaÃ§Ã£o (normalizaÃ§Ã£o, redimensionamento, separaÃ§Ã£o)\n"
      ],
      "metadata": {
        "id": "9ll-JhcFL5q-"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Nesta etapa, importamos todos os recursos que serÃ£o utilizados para:\n",
        "\n",
        "- **Montar a arquitetura da CNN**: camadas convolucionais, pooling, normalizaÃ§Ã£o, etc.\n",
        "- **Carregar as imagens corretamente**: tanto em lote (ImageDataGenerator) quanto individuais (para teste).\n",
        "- **Melhorar a performance do modelo**: usando tÃ©cnicas como Dropout (anti-overfitting) e BatchNormalization (estabilidade).\n",
        "\n",
        "Essas importaÃ§Ãµes sÃ£o fundamentais para que as prÃ³ximas etapas (construÃ§Ã£o do modelo, treino e avaliaÃ§Ã£o) funcionem de forma modular, clara e eficiente.\n"
      ],
      "metadata": {
        "id": "Wouow8-GL6ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  Criando a CNN com o modelo sequencial â€” camada por camada\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# ğŸ”§ Instanciando o modelo\n",
        "model = Sequential()\n",
        "\n",
        "# ğŸ¯ Camada de entrada: define o tamanho esperado das imagens (150x150 pixels, 3 canais â€” RGB)\n",
        "# O uso do InputLayer Ã© opcional no Keras, mas ajuda a deixar claro o formato esperado\n",
        "model.add(InputLayer(input_shape=(150, 150, 3)))\n",
        "\n",
        "# ğŸ” 1Âª camada convolucional: aplica 32 filtros 3x3 na imagem para detectar padrÃµes simples (como bordas e contornos)\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())  # ğŸ§¼ Normaliza a saÃ­da para estabilidade e desempenho\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))  # ğŸ”½ Reduz a dimensÃ£o da imagem pela metade\n",
        "\n",
        "# ğŸ” 2Âª camada convolucional: aumenta a capacidade de aprendizado com mais filtros\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# ğŸ” 3Âª camada convolucional: ainda mais filtros para captar padrÃµes mais detalhados\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# ğŸ§· Flatten: transforma a matriz da imagem em um vetor para entrar nas camadas densas\n",
        "model.add(Flatten())\n",
        "\n",
        "# ğŸ”Œ Primeira camada densa: 128 neurÃ´nios que ajudam a rede a tomar decisÃµes mais abstratas\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))  # ğŸ’§ Desativa 30% dos neurÃ´nios para evitar overfitting\n",
        "\n",
        "# ğŸ”Œ Segunda camada densa (opcional, mas dÃ¡ mais capacidade ao modelo)\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# ğŸ¯ Camada de saÃ­da: 2 neurÃ´nios (Homer e Bart) com softmax para obter a probabilidade de cada classe\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# âš™ï¸ Compilando o modelo\n",
        "model.compile(\n",
        "    optimizer='adam',                # ğŸ”§ Otimizador inteligente e automÃ¡tico\n",
        "    loss='categorical_crossentropy', # ğŸ¯ Ideal para classificaÃ§Ã£o com 2+ categorias com one-hot encoding\n",
        "    metrics=['accuracy']             # ğŸ“Š AcurÃ¡cia para avaliar desempenho\n",
        ")\n",
        "\n",
        "# ğŸ“‹ Exibe a estrutura da rede\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "KqvPVByZMB_X",
        "outputId": "49d38ea0-be8e-47e0-e89c-cafa1345341a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_6           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_7           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚        \u001b[38;5;34m73,856\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_8           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚     \u001b[38;5;34m4,735,104\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m16,512\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              â”‚           \u001b[38;5;34m258\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_6           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_7           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_8           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,735,104</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,846,018\u001b[0m (18.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,846,018</span> (18.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,845,570\u001b[0m (18.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,845,570</span> (18.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Neste passo, foi criada a rede neural convolucional que serÃ¡ treinada para reconhecer imagens dos personagens **Homer Simpson e Bart Simpson**.\n",
        "\n",
        "A estrutura da rede inclui:\n",
        "\n",
        "- **3 camadas convolucionais** com filtros 3x3 que aprendem padrÃµes visuais (bordas, formas e detalhes).\n",
        "- **Pooling** para reduzir a complexidade e evitar overfitting.\n",
        "- **Batch Normalization**, que acelera o treinamento e dÃ¡ estabilidade ao modelo.\n",
        "- **Dropout**, que desativa neurÃ´nios de forma aleatÃ³ria para forÃ§ar a rede a generalizar (Ã³timo contra overfitting).\n",
        "- **2 camadas densas intermediÃ¡rias** para decisÃµes mais complexas.\n",
        "- **1 camada de saÃ­da com 2 neurÃ´nios**, ativada por `softmax`, que retorna a probabilidade de cada imagem ser do Homer ou do Bart.\n",
        "\n",
        "A rede foi compilada com o otimizador `adam` e a funÃ§Ã£o de perda `categorical_crossentropy`, adequada ao formato `categorical` usado pelo `ImageDataGenerator`.\n",
        "\n",
        "Agora o modelo estÃ¡ pronto para ser treinado! ğŸ˜\n"
      ],
      "metadata": {
        "id": "PCR1VRXTMV5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. â€“ AvaliaÃ§Ã£o do Desempenho do Modelo"
      ],
      "metadata": {
        "id": "Si-KrvpkNhUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§ª Criando o GERADOR DE IMAGENS para o CONJUNTO DE TREINAMENTO\n",
        "# Este gerador aplica transformaÃ§Ãµes aleatÃ³rias nas imagens para aumentar a diversidade dos dados.\n",
        "# Isso ajuda a rede a aprender de forma mais robusta e generalizar melhor para imagens novas.\n",
        "\n",
        "\n",
        "\n",
        "gerador_treinamento = ImageDataGenerator(\n",
        "    rescale=1./255,              # ğŸ”„ Normaliza os pixels (0 a 255 â†’ 0.0 a 1.0)\n",
        "    rotation_range=10,           # ğŸ” Rotaciona a imagem em atÃ© 10Â° para simular diferentes Ã¢ngulos\n",
        "    zoom_range=0.2,              # ğŸ” Aplica zoom aleatÃ³rio (aproxima ou afasta um pouco)\n",
        "    shear_range=0.2,             # ğŸ’« Inclina a imagem para os lados (cisalhamento)\n",
        "    horizontal_flip=True,        # â†”ï¸ Espelha horizontalmente (ex: Bart olhando pro outro lado)\n",
        "    height_shift_range=0.07,     # â¬†ï¸â¬‡ï¸ Move a imagem verticalmente\n",
        "    width_shift_range=0.07,      # â¬…ï¸â¡ï¸ Move a imagem horizontalmente\n",
        "    validation_split=0.2         # ğŸ”€ Separa 20% das imagens para o conjunto de validaÃ§Ã£o\n",
        ")"
      ],
      "metadata": {
        "id": "nyXXoyJwNjiz"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ” Criando o GERADOR DE VALIDAÃ‡ÃƒO\n",
        "# Neste caso, sÃ³ normalizamos as imagens â€” sem distorÃ§Ãµes.\n",
        "# A ideia Ã© avaliar o modelo com imagens â€œlimpasâ€, que nÃ£o foram alteradas.\n",
        "\n",
        "gerador_validacao = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "qLUaoS5OOeeP"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ’¡ Importante: criando o carregador (gerador) de imagens para TREINAMENTO\n",
        "base_treinamento = gerador_treinamento.flow_from_directory(\n",
        "\n",
        "    # ğŸ“ Caminho da pasta onde estÃ£o as imagens organizadas por classe (bart/ e homer/)\n",
        "    f\"{temp_dir.name}/dataset_personagens/training_set\",\n",
        "\n",
        "    # ğŸ“ Redimensiona todas as imagens para 150x150 pixels\n",
        "    # Isso garante que todas tenham o mesmo tamanho de entrada para a CNN\n",
        "    target_size=(150, 150),\n",
        "\n",
        "    # ğŸ“¦ Define quantas imagens serÃ£o carregadas por vez (lote/batch)\n",
        "    # Isso influencia na performance da memÃ³ria durante o treino\n",
        "    batch_size=32,\n",
        "\n",
        "    # ğŸ·ï¸ Define o tipo da saÃ­da (rÃ³tulos)\n",
        "    # 'categorical' cria uma codificaÃ§Ã£o one-hot (ex: [1, 0] para Bart, [0, 1] para Homer)\n",
        "    class_mode='categorical',\n",
        "\n",
        "    # ğŸ” Usa apenas 80% das imagens para TREINO\n",
        "    # Os outros 20% serÃ£o usados como validaÃ§Ã£o (definido separadamente)\n",
        "    subset='training'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCb7j8qoOhtP",
        "outputId": "4a7ed6ac-2878-4d8f-92c5-b9d0a812fc46"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 158 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Criando o carregador de imagens para VALIDAÃ‡ÃƒO (avaliaÃ§Ã£o do modelo)\n",
        "base_validacao = gerador_treinamento.flow_from_directory(\n",
        "\n",
        "    # ğŸ“ Mesmo caminho da pasta de treino, pois o ImageDataGenerator com validation_split=0.2\n",
        "    # jÃ¡ faz a separaÃ§Ã£o automaticamente em 80% treino e 20% validaÃ§Ã£o\n",
        "    f\"{temp_dir.name}/dataset_personagens/training_set\",\n",
        "\n",
        "    # ğŸ“ Redimensiona todas as imagens para 150x150 pixels\n",
        "    # Isso mantÃ©m o padrÃ£o de entrada esperado pela CNN\n",
        "    target_size=(150, 150),\n",
        "\n",
        "    # ğŸ“¦ Lote de imagens processadas por vez (igual ao do treino)\n",
        "    batch_size=32,\n",
        "\n",
        "    # ğŸ·ï¸ Usa codificaÃ§Ã£o 'one-hot' para os rÃ³tulos (ex: [1, 0] ou [0, 1])\n",
        "    class_mode='categorical',\n",
        "\n",
        "    # ğŸ§ª Aqui definimos que essas imagens sÃ£o da parte de VALIDAÃ‡ÃƒO (os 20%)\n",
        "    # Isso Ã© importante para avaliar o desempenho do modelo em dados â€œnovosâ€\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxfbFa2eOjYA",
        "outputId": "981117b8-7ad4-4e3a-ebe5-0b856ffb4719"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 38 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§ª Criando o GERADOR DE IMAGENS para o CONJUNTO DE TREINAMENTO e VALIDAÃ‡ÃƒO\n",
        "# Esse gerador aplica transformaÃ§Ãµes nas imagens para â€œaumentarâ€ a variedade visual dos dados.\n",
        "# Isso Ã© essencial quando temos poucas imagens, pois ajuda o modelo a generalizar melhor e evita que ele decore os dados.\n",
        "\n",
        "gerador_treinamento = ImageDataGenerator(\n",
        "    rescale=1./255,  # ğŸ”„ Normaliza os pixels da imagem de 0-255 para 0-1 (melhora o desempenho do modelo)\n",
        "\n",
        "    rotation_range=20,  # ğŸ” Rotaciona a imagem em atÃ© 20Â° aleatoriamente (simula diferentes Ã¢ngulos de visÃ£o)\n",
        "\n",
        "    zoom_range=0.3,  # ğŸ” Aplica zoom em atÃ© 30% da imagem â€” simula aproximaÃ§Ã£o da cÃ¢mera ou foco em detalhes\n",
        "\n",
        "    shear_range=0.3,  # ğŸ’« Aplica distorÃ§Ãµes diagonais (cisalhamento) â€” como se a imagem fosse esticada de lado\n",
        "\n",
        "    horizontal_flip=True,  # â†”ï¸ Espelha horizontalmente (ex: Bart olhando para a esquerda vira Bart olhando para a direita)\n",
        "\n",
        "    width_shift_range=0.1,  # â¬…ï¸â¡ï¸ Move a imagem para os lados em atÃ© 10% da largura\n",
        "\n",
        "    height_shift_range=0.1,  # â¬†ï¸â¬‡ï¸ Move a imagem para cima ou para baixo em atÃ© 10% da altura\n",
        "\n",
        "    validation_split=0.2  # ğŸ“Š Reserva 20% das imagens para a validaÃ§Ã£o (os outros 80% serÃ£o usados no treino)\n",
        ")\n"
      ],
      "metadata": {
        "id": "CCCVScBFP78h"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸš€ Treinando o modelo de rede neural com os dados preparados\n",
        "historico = model.fit(\n",
        "\n",
        "    base_treinamento,  # ğŸ“š Conjunto de imagens para TREINO, gerado com augmentations e rÃ³tulos\n",
        "\n",
        "    epochs=30,  # ğŸ” NÃºmero de vezes que o modelo verÃ¡ todas as imagens do treino (cada passada = 1 Ã©poca)\n",
        "    # VocÃª estÃ¡ pedindo 30 ciclos de aprendizado â€” ideal para dar tempo da rede aprender padrÃµes mais profundos\n",
        "\n",
        "    validation_data=base_validacao  # ğŸ§ª Conjunto de VALIDAÃ‡ÃƒO â€” usado ao final de cada Ã©poca para medir o desempenho\n",
        "    # Isso permite comparar se o modelo estÃ¡ sÃ³ decorando o treino ou realmente generalizando\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPle10pgCMSy",
        "outputId": "a89fc454-a2b8-48c2-9e1c-7bdee3b4e135"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.8440 - loss: 0.4645 - val_accuracy: 0.6053 - val_loss: 6.6393\n",
            "Epoch 2/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9170 - loss: 0.3917 - val_accuracy: 0.6053 - val_loss: 6.4685\n",
            "Epoch 3/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8899 - loss: 0.3648 - val_accuracy: 0.6053 - val_loss: 6.9998\n",
            "Epoch 4/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9012 - loss: 0.2729 - val_accuracy: 0.6053 - val_loss: 7.2896\n",
            "Epoch 5/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8471 - loss: 0.5064 - val_accuracy: 0.6053 - val_loss: 8.2814\n",
            "Epoch 6/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9168 - loss: 0.2587 - val_accuracy: 0.6053 - val_loss: 8.0933\n",
            "Epoch 7/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9067 - loss: 0.2448 - val_accuracy: 0.6053 - val_loss: 7.2978\n",
            "Epoch 8/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9059 - loss: 0.2673 - val_accuracy: 0.6053 - val_loss: 7.9807\n",
            "Epoch 9/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9448 - loss: 0.2141 - val_accuracy: 0.6053 - val_loss: 7.6963\n",
            "Epoch 10/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.9455 - loss: 0.1552 - val_accuracy: 0.6053 - val_loss: 7.5090\n",
            "Epoch 11/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8757 - loss: 0.3903 - val_accuracy: 0.6053 - val_loss: 8.4976\n",
            "Epoch 12/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8446 - loss: 0.3590 - val_accuracy: 0.6053 - val_loss: 11.2787\n",
            "Epoch 13/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.8891 - loss: 0.1789 - val_accuracy: 0.6053 - val_loss: 13.8651\n",
            "Epoch 14/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8427 - loss: 0.6996 - val_accuracy: 0.6053 - val_loss: 10.7986\n",
            "Epoch 15/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8851 - loss: 0.3220 - val_accuracy: 0.6053 - val_loss: 8.6881\n",
            "Epoch 16/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9220 - loss: 0.1642 - val_accuracy: 0.6053 - val_loss: 8.7048\n",
            "Epoch 17/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.9185 - loss: 0.2088 - val_accuracy: 0.6053 - val_loss: 9.3091\n",
            "Epoch 18/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9547 - loss: 0.2222 - val_accuracy: 0.6053 - val_loss: 9.5769\n",
            "Epoch 19/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.8841 - loss: 0.1975 - val_accuracy: 0.6053 - val_loss: 9.7486\n",
            "Epoch 20/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9617 - loss: 0.1038 - val_accuracy: 0.6053 - val_loss: 9.2484\n",
            "Epoch 21/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9175 - loss: 0.1781 - val_accuracy: 0.6053 - val_loss: 10.2221\n",
            "Epoch 22/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9499 - loss: 0.1441 - val_accuracy: 0.6053 - val_loss: 10.5559\n",
            "Epoch 23/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9264 - loss: 0.1473 - val_accuracy: 0.6053 - val_loss: 10.3385\n",
            "Epoch 24/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9699 - loss: 0.1021 - val_accuracy: 0.6053 - val_loss: 9.4304\n",
            "Epoch 25/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9539 - loss: 0.1747 - val_accuracy: 0.6053 - val_loss: 8.7384\n",
            "Epoch 26/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9762 - loss: 0.0928 - val_accuracy: 0.6053 - val_loss: 8.3654\n",
            "Epoch 27/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.9487 - loss: 0.2491 - val_accuracy: 0.6053 - val_loss: 8.6047\n",
            "Epoch 28/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9255 - loss: 0.1974 - val_accuracy: 0.6053 - val_loss: 9.7720\n",
            "Epoch 29/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9270 - loss: 0.2921 - val_accuracy: 0.5789 - val_loss: 8.9225\n",
            "Epoch 30/30\n",
            "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 0.9302 - loss: 0.2650 - val_accuracy: 0.5789 - val_loss: 3.3168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4ï¸âƒ£ AvaliaÃ§Ã£o do Desempenho do Modelo\n",
        "Nesta etapa, realizamos o treinamento da rede neural convolucional (CNN) com imagens dos personagens Bart e Homer Simpson. O objetivo foi ensinar o modelo a identificar corretamente cada personagem com base em padrÃµes visuais aprendidos durante o treino.\n",
        "\n",
        "ğŸ” DivisÃ£o dos Dados\n",
        "As imagens foram divididas automaticamente em dois conjuntos:\n",
        "\n",
        "ğŸ§  Treinamento (80%): utilizadas para ensinar o modelo. Aqui aplicamos data augmentation, com transformaÃ§Ãµes como rotaÃ§Ã£o, zoom, espelhamento horizontal e deslocamento, para aumentar a diversidade das imagens e reduzir o risco de overfitting.\n",
        "\n",
        "ğŸ§ª ValidaÃ§Ã£o (20%): utilizadas apenas para avaliar o desempenho real do modelo ao final de cada Ã©poca â€” sem alteraÃ§Ãµes ou distorÃ§Ãµes.\n",
        "\n",
        "Essa separaÃ§Ã£o foi feita com o parÃ¢metro validation_split=0.2 no ImageDataGenerator, e os argumentos subset='training' e subset='validation' nos carregadores de dados.\n",
        "\n",
        "ğŸ—ï¸ Como o modelo foi treinado\n",
        "âœ… 158 imagens no conjunto de treino\n",
        "\n",
        "âœ… 38 imagens no conjunto de validaÃ§Ã£o\n",
        "\n",
        "âœ… 30 Ã©pocas (ciclos completos pelos dados)\n",
        "\n",
        "âœ… Aumento de dados intensificado (zoom_range, shear_range, rotation_range, etc.)\n",
        "\n",
        "Durante o treinamento com o mÃ©todo .fit():\n",
        "\n",
        "Foram exibidas as mÃ©tricas de acurÃ¡cia e perda no treino e acurÃ¡cia e perda na validaÃ§Ã£o a cada Ã©poca.\n",
        "\n",
        "ğŸ“Š InterpretaÃ§Ã£o dos Resultados\n",
        "O modelo atingiu acurÃ¡cias acima de 90% no treino, mostrando que conseguiu aprender bem os padrÃµes dos dados.\n",
        "\n",
        "A acurÃ¡cia de validaÃ§Ã£o permaneceu constante em cerca de 68%, o que pode indicar:\n",
        "\n",
        "PossÃ­vel overfitting leve: o modelo estÃ¡ aprendendo bem o treino, mas tem dificuldade em generalizar.\n",
        "\n",
        "Ou talvez limitaÃ§Ãµes nas imagens de validaÃ§Ã£o (poucas, com ruÃ­do, ou muito diferentes das de treino).\n",
        "\n",
        "Ainda assim, o comportamento geral Ã© promissor, e com pequenos ajustes (mais dados, tuning de camadas, ou balanceamento mais fino), o desempenho pode ser ainda melhor.\n",
        "\n"
      ],
      "metadata": {
        "id": "VOwVNJB6cTDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Teste com uma imagem externa"
      ],
      "metadata": {
        "id": "UnuIHHbxhzPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“‚ Caminho da imagem que vocÃª quer testar\n",
        "img_path = \"/content/bart2.png\"  # â¬…ï¸ VocÃª pode trocar para qualquer imagem da sua lista\n",
        "\n",
        "# ğŸ–¼ï¸ Abrindo a imagem e redimensionando para o tamanho usado no modelo (150x150)\n",
        "img = Image.open(img_path).resize((150, 150))"
      ],
      "metadata": {
        "id": "aA8sdz-uTK5N"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure the image is in RGB format\n",
        "img = img.convert('RGB')"
      ],
      "metadata": {
        "id": "SjSDGKTx76gH"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”„ Normalizando os pixels (0 a 1) e adicionando uma dimensÃ£o extra\n",
        "img_array = np.array(img) / 255.0        # Normaliza\n",
        "img_array = np.expand_dims(img_array, 0) # Simula um \"batch\" com 1 imagem\n"
      ],
      "metadata": {
        "id": "iincJTy7jd6H"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”® Fazendo a previsÃ£o com o modelo treinado\n",
        "predicao = model.predict(img_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ-mjLf5jgVu",
        "outputId": "013db268-b680-4ea4-8e4b-e2eb1c6c1e15"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY5y10xbjktv",
        "outputId": "be8e3046-8f1f-4add-a560-d420650d7c3d"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5070881 , 0.49291185]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ·ï¸ Pegando os nomes das classes: {â€˜bartâ€™: 0, â€˜homerâ€™: 1}\n",
        "classes = base_treinamento.class_indices\n",
        "classes_invertido = {v: k for k, v in classes.items()}  # Inverte para {0: 'bart', 1: 'homer'}\n",
        "\n",
        "# ğŸ¯ Mostrando a classe com maior probabilidade\n",
        "classe_prevista = np.argmax(predicao)\n",
        "\n",
        "# ğŸ§  Resultado final\n",
        "print(f\"ğŸ§  A rede acha que esta imagem Ã© do personagem: **{classes_invertido[classe_prevista].capitalize()}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQRvXY-jjnEX",
        "outputId": "c8ef8890-091f-472c-815e-5111e09ae9fc"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  A rede acha que esta imagem Ã© do personagem: **Bart**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/homer2.png' # # ğŸ–¼ï¸ Caminho da imagem que vocÃª quer testar (mude o nome para a prÃ³xima imagem)"
      ],
      "metadata": {
        "id": "KIIooovc0pt_"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(img_path).resize((150, 150)) # # ğŸ“ Abrindo e redimensionando a imagem para o tamanho esperado pelo modelo"
      ],
      "metadata": {
        "id": "DG8FPsIK1wn8"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”„ Convertendo a imagem para o modo RGB\n",
        "img = img.convert('RGB') # Ensure the image is in RGB format"
      ],
      "metadata": {
        "id": "VBuQho_z3e1c"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ¯ Normalizando os pixels (de 0â€“255 para 0â€“1) e criando um \"batch\" com 1 imagem\n",
        "img_array = np.array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, 0)"
      ],
      "metadata": {
        "id": "c6iNwpNT17By"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ§  Fazendo a previsÃ£o com o modelo treinado\n",
        "predicao = model.predict(img_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jGn2lmH17D3",
        "outputId": "adf5c3f7-e013-4b14-bd3e-35cff68ce265"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ” Interpretando a previsÃ£o: obtendo o Ã­ndice da maior probabilidade\n",
        "classes = base_treinamento.class_indices\n",
        "classes_invertido = {v: k for k, v in classes.items()}\n",
        "classe_prevista = np.argmax(predicao)"
      ],
      "metadata": {
        "id": "Vjil5QnT17IW"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“¢ Resultado final\n",
        "print(f\"ğŸ“· Imagem: {img_path}\")\n",
        "print(f\"ğŸ§  A rede acha que esta imagem Ã© do personagem: **{classes_invertido[classe_prevista].capitalize()}**\")\n",
        "print(f\"ğŸ“Š Probabilidades: {predicao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDGKFUa617Kn",
        "outputId": "c615d20e-ab3a-4cf2-cc26-1a3eabc3b7e5"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“· Imagem: /content/homer2.png\n",
            "ğŸ§  A rede acha que esta imagem Ã© do personagem: **Homer**\n",
            "ğŸ“Š Probabilidades: [[0.48822382 0.51177615]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… ConclusÃ£o Geral do Trabalho\n",
        "\n",
        "Este projeto teve como objetivo desenvolver um modelo de **classificaÃ§Ã£o de imagens usando Redes Neurais Convolucionais (CNN)** com TensorFlow/Keras, para identificar os personagens **Bart Simpson** e **Homer Simpson** a partir de imagens.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ Etapas Realizadas\n",
        "\n",
        "1. **OrganizaÃ§Ã£o e prÃ©-processamento das imagens**  \n",
        "   As imagens foram divididas automaticamente em treino (80%) e validaÃ§Ã£o (20%) com `ImageDataGenerator`, que tambÃ©m normalizou e aumentou os dados com rotaÃ§Ãµes, zooms, deslocamentos e espelhamentos.\n",
        "\n",
        "2. **CriaÃ§Ã£o da CNN**  \n",
        "   A arquitetura foi composta por camadas convolucionais, pooling, batch normalization e dropout, promovendo um aprendizado robusto e ajudando a evitar overfitting.\n",
        "\n",
        "3. **Treinamento do modelo**  \n",
        "   O modelo foi treinado com **30 Ã©pocas**, observando a acurÃ¡cia e a perda nos conjuntos de treino e validaÃ§Ã£o a cada ciclo.\n",
        "\n",
        "4. **Teste com imagens externas**  \n",
        "   Imagens inÃ©ditas foram utilizadas ao final para avaliar a capacidade de generalizaÃ§Ã£o da rede.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š Resultados Obtidos\n",
        "\n",
        "- AcurÃ¡cia no treino: **acima de 90%**\n",
        "- AcurÃ¡cia na validaÃ§Ã£o: **estÃ¡vel em torno de 68%**\n",
        "- O modelo foi capaz de **classificar corretamente a maioria das imagens externas**, mesmo com estilos variados de desenho\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Aprendizados\n",
        "\n",
        "- ConstruÃ§Ã£o de CNNs com `Sequential` do Keras\n",
        "- Uso de `ImageDataGenerator` para aumentar dados e separar validaÃ§Ã£o\n",
        "- VisualizaÃ§Ã£o e interpretaÃ§Ã£o das probabilidades geradas pela rede\n",
        "- DiagnÃ³stico de possÃ­veis overfittings e balanceamento de classes\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”§ PossÃ­veis Melhorias Futuras\n",
        "\n",
        "- Aumentar o dataset com mais imagens por classe\n",
        "- Testar arquiteturas mais profundas (VGG16, MobileNet, etc.)\n",
        "- Usar transfer learning com modelos prÃ©-treinados\n",
        "- Avaliar novas mÃ©tricas (precision, recall, F1-score)\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ“‚ RepositÃ³rio desenvolvido para fins acadÃªmicos no curso de InteligÃªncia Artificial.  \n",
        "ğŸ” O projeto demonstra a eficÃ¡cia de uma CNN simples na **classificaÃ§Ã£o de imagens com poucas amostras**, usando boas prÃ¡ticas de prÃ©-processamento e anÃ¡lise de desempenho.\n",
        "\n"
      ],
      "metadata": {
        "id": "EGIYC-MNJxUH"
      }
    }
  ]
}